{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PostRnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB_MuuS5KuR1",
        "outputId": "44276574-51d3-4e89-f58b-3baae175d558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# install necessary packages using pip\n",
        "!pip install keras numpy wget"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=21e9757e8f306c0e74408822c3fac24b0501b581a67efabf88f63c9e28d8e7e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU-a8rThaHWN"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import sys\n",
        "import wget"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OwMB1QbMOlF"
      },
      "source": [
        "# NOTE*: The train.zip file contains all 500 (unzipped) documents, \n",
        "#   so it only has to be unzipped once.\n",
        "\n",
        "fileName = \"/content/train.zip\"\n",
        "\n",
        "# Checks if the file already exists.\n",
        "if os.path.isfile(fileName):\n",
        "    print(\"'\" + fileName + \"' already exists.\")\n",
        "\n",
        "# Otherwise, it downloads it.    \n",
        "else:\n",
        "    url = \"https://raw.githubusercontent.com/rks170530/NLP_Project1_Corpus/main/train.zip\"\n",
        "    wget.download(url, fileName)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "029-d_BBn3ww"
      },
      "source": [
        "# NOTE**: In case the wget.download() does not work for whatever reason, \n",
        "#   please use the \"train.zip\" file included in the Project1 folder, uncomment\n",
        "#   the next 3 lines and upload the file when prompted.\n",
        "\n",
        "# from google.colab import files\n",
        "# files.upload()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSQcBy5fpQ6B"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "# Extract the Zipfile into the 500 documents.\n",
        "with ZipFile(fileName, 'r') as zip:\n",
        "  zip.extractall()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT37l9LMoGYx",
        "outputId": "7d027bcf-9985-4cd6-db50-16cbd9cc171b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "def load_corpus(path):\n",
        "\n",
        "    # Check if the path is a directory.\n",
        "    if not os.path.isdir(path):\n",
        "        sys.exit(\"Input path is not a directory\")\n",
        "\n",
        "    sentenceArray = []\n",
        "\n",
        "    # For each file in the path.\n",
        "    for fileName in os.listdir(path):\n",
        "        fileName = os.path.join(path, fileName)\n",
        "    \n",
        "        # Open and read the file.\n",
        "        try:\n",
        "            fileReader = io.open(fileName, 'r', encoding='utf-8', errors='ignore')\n",
        "\n",
        "            # Until there are no more lines.\n",
        "            while True:\n",
        "                    \n",
        "                # Read the line.\n",
        "                wordArray = []\n",
        "                currentLine = fileReader.readline()\n",
        "\n",
        "                # Exit if there are no new lines.\n",
        "                if not currentLine:\n",
        "                    break\n",
        "                \n",
        "                # Remove end-line and convert to lowercase.\n",
        "                currentLine = currentLine.replace('\\n', \"\")\n",
        "\n",
        "                # Save the line only if it isn't empty.\n",
        "                if (currentLine != \"\"):\n",
        "\n",
        "                    # Append every word in the line into the word array.\n",
        "                    for word in currentLine.split(' '):\n",
        "                        \n",
        "                        # Split the word and tag.\n",
        "                        try:\n",
        "                            tokenTuple = word.split('/')\n",
        "                            wordArray.append((tokenTuple[0], tokenTuple[1]))\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                    # print(wordArray)\n",
        "                    sentenceArray.append(wordArray)\n",
        "\n",
        "        # Exit otherwise.\n",
        "        except IOError:\n",
        "            sys.exit(\"Cannot read file\")\n",
        "\n",
        "    return sentenceArray\n",
        "\n",
        "# test the function here:\n",
        "path = \"/content/train/\"\n",
        "data = load_corpus(path)\n",
        "print(data[0])\n",
        "print(len(data))\n",
        "print(len(data[0]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Local', 'ADJECTIVE'), (\"industry's\", 'NOUN'), ('investment', 'NOUN'), ('in', 'PREPOSITION'), ('Rhode', 'NOUN'), ('Island', 'NOUN'), ('was', 'VERB'), ('the', 'DETERMINER'), ('big', 'ADJECTIVE'), ('story', 'NOUN'), ('in', 'PREPOSITION'), (\"1960's\", 'NUMBER'), ('industrial', 'ADJECTIVE'), ('development', 'NOUN'), ('effort', 'NOUN'), ('.', 'PUNCT')]\n",
            "57340\n",
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFJvfGCPois_",
        "outputId": "63cb67bd-b290-4e2e-c14a-4c98b10f1cac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import numpy as np # convert lists to np arrays before returning them\n",
        "\n",
        "# Creates the dataset with train_X (words) and train_y (tag).\n",
        "def create_dataset(sentences):\n",
        "    \n",
        "    # Defines the relevant lists.\n",
        "    train_X, train_y = list(), list()\n",
        "\n",
        "    # Dictionaries that will provide word/tag to integer mapping\n",
        "    word2idx, tag2idx = dict(), dict() \n",
        "\n",
        "    # Dictionary to convert ids back to the respected words.\n",
        "    id2wordX = {}\n",
        "    uniqueWordCount = 0\n",
        "\n",
        "    # Include a [PAD] token for the lists that aren't the MAX_LENGTH in size. \n",
        "    word2idx[\"[PAD]\"] = uniqueWordCount\n",
        "    id2wordX[uniqueWordCount] = \"[PAD]\"\n",
        "    uniqueWordCount += 1\n",
        "\n",
        "    # Include a [OOV] token for words that can't be found in the dictionary.\n",
        "    word2idx[\"[OOV]\"] = uniqueWordCount\n",
        "    id2wordX[uniqueWordCount] = \"[OOV]\"\n",
        "    uniqueWordCount += 1\n",
        "\n",
        "    # Go through each sentence in the data.\n",
        "    for s in data:\n",
        "        # For each word.\n",
        "        for w in s:\n",
        "\n",
        "            # Convert to lowercase.\n",
        "            word = w[0].lower()\n",
        "\n",
        "            # Add it to the word2idx and id2wordx dictionaries.\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = uniqueWordCount\n",
        "                id2wordX[uniqueWordCount] = word\n",
        "                uniqueWordCount += 1\n",
        "    \n",
        "    # The tag possibilities list.\n",
        "    tagString = \"NOUN, PRONOUN, VERB, ADVERB, ADJECTIVE, CONJUNCTION, PREPOSITION, DETERMINER, NUMBER, PUNCT, X\"\n",
        "    tagArray = tagString.split(', ')\n",
        "\n",
        "    # Dictionary to convert ids back to the respected tags.\n",
        "    id2tagX = {}\n",
        "    uniqueTagCount = 0\n",
        "\n",
        "    # Include a [PAD] token for the lists that aren't the MAX_LENGTH in size. \n",
        "    tag2idx[\"[PAD]\"] = uniqueTagCount\n",
        "    id2tagX[uniqueTagCount] = \"[PAD]\"\n",
        "    uniqueTagCount += 1\n",
        "\n",
        "    # For each tag in the tag list.\n",
        "    for tag in tagArray:\n",
        "\n",
        "        # Add it to the tag2idx and id2tagx dictionaries if it doesn't exist.\n",
        "        if tag not in tag2idx:\n",
        "            tag2idx[tag] = uniqueTagCount\n",
        "            id2tagX[uniqueTagCount] = tag\n",
        "            uniqueTagCount += 1\n",
        "\n",
        "    # Deep copy the sentences array for train_X and train_y.\n",
        "    train_X = [row[:] for row in sentences]\n",
        "    train_y = [row[:] for row in sentences]\n",
        "\n",
        "    # For each sentence in the sentences list.\n",
        "    for s in range(len(sentences)):\n",
        "        \n",
        "        # For each word in the sentence.\n",
        "        for w in range(len(sentences[s])):\n",
        "\n",
        "            # Get the (lowercase) word and tag, add it to train_X and train_y.\n",
        "            word = sentences[s][w][0].lower()\n",
        "            tag = sentences[s][w][1]\n",
        "            train_X[s][w] = word2idx[word]\n",
        "            train_y[s][w] = tag2idx[tag]\n",
        "\n",
        "    # Convert into a numpy array.\n",
        "    train_X = np.array(train_X)\n",
        "    train_y = np.array(train_y)\n",
        "\n",
        "    # Output train_X, train_y, and the relevant dictionries.\n",
        "    return train_X, train_y, word2idx, tag2idx, id2wordX, id2tagX \n",
        "\n",
        "# Test the function\n",
        "train_X, train_y, word2idx, tag2idx, id2wordX, id2tagX = create_dataset(data)\n",
        "print(train_X[0])\n",
        "print(train_y[0])\n",
        "print(id2wordX[train_X[0][0]])\n",
        "print(id2tagX[train_y[0][0]])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 5, 12, 13, 14, 15, 16]\n",
            "[5, 1, 1, 7, 1, 1, 3, 8, 5, 1, 7, 9, 5, 1, 1, 10]\n",
            "local\n",
            "ADJECTIVE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk0ZTHkvplxD",
        "outputId": "e0d53cbc-903c-4585-c6e2-6c3e2a179c76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences as pad\n",
        "\n",
        "# Pad the sequences with 0s to the max length.\n",
        "def pad_sequences(train_X, train_y):\n",
        "\n",
        "    # Used to find the max array size.\n",
        "    tempMax = -1\n",
        "\n",
        "    # Keep and store the max length of a sentence as it goes through train_X.\n",
        "    for s in train_X:\n",
        "        if (len(s) > tempMax):\n",
        "            tempMax = len(s)\n",
        "\n",
        "    # Define the MAX_LENGTH of a sentence.\n",
        "    MAX_LENGTH = tempMax\n",
        "\n",
        "    # Pad train_X and train_y.\n",
        "    train_X = pad(train_X, MAX_LENGTH, 'int32', 'post', 'pre', 0.0)\n",
        "    train_y = pad(train_y, MAX_LENGTH, 'int32', 'post', 'pre', 0.0)\n",
        "    \n",
        "    # Return the numpy arrays and MAX-LENGTH.\n",
        "    return train_X, train_y, MAX_LENGTH\n",
        "\n",
        "# Test the function\n",
        "train_X, train_y, MAX_LENGTH = pad_sequences(train_X, train_y)\n",
        "print(train_X[0])\n",
        "print(train_y[0])\n",
        "print(MAX_LENGTH)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2  3  4  5  6  7  8  9 10 11  5 12 13 14 15 16  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "[ 5  1  1  7  1  1  3  8  5  1  7  9  5  1  1 10  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edy9gTV6qIhv",
        "outputId": "2247876f-fe4e-44a3-eeec-d39715de704a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import InputLayer, Activation\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Define the Keras model.\n",
        "def define_model(MAX_LENGTH):  \n",
        "    model = Sequential()\n",
        "    model.add(InputLayer(input_shape=(MAX_LENGTH, ))) # MAX_LENGTH is the max length of each sequence, as output by previous method\n",
        "\n",
        "    # Adds an Embedding layer, Bidirectional layer, and is a Dense netowrk.\n",
        "    model.add(Embedding(len(word2idx), 128))\n",
        "    model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(len(tag2idx))))\n",
        "\n",
        "    # Add the softmax activation and the Adam optimizer for loss.\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
        "  \n",
        "    # Print the model summary.\n",
        "    print (model.summary())\n",
        "    return model\n",
        "\n",
        "# Call the function here\n",
        "model = define_model(MAX_LENGTH)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 180, 128)          6367104   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 180, 512)          788480    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 180, 12)           6156      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 180, 12)           0         \n",
            "=================================================================\n",
            "Total params: 7,161,740\n",
            "Trainable params: 7,161,740\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0c2a7eUPQFi",
        "outputId": "57586749-b744-4af7-bca1-5c19f210b8dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "# Returns the one-hot encoding of the sequence.\n",
        "def to_categorical(sequences, categories = 11):\n",
        "\n",
        "    # Creates the sentence list.\n",
        "    sentenceList = []\n",
        "\n",
        "    # For each sentence in the seqeuences list.\n",
        "    for s in range(len(sequences)):\n",
        "\n",
        "        # Create a word list for each sentence.\n",
        "        wordList = []\n",
        "\n",
        "        # For each word in the sentence.\n",
        "        for w in range(len(sequences[s])):\n",
        "\n",
        "            # Get the integer value of the word.\n",
        "            tempValue = sequences[s][w]\n",
        "\n",
        "            # Create list with with 0s the length of the categories.\n",
        "            tempList = np.zeros(categories)\n",
        "\n",
        "            # Fill the 1 in the relevant 1-hot position and append it to\n",
        "            # the word List.\n",
        "            tempList[tempValue] = 1\n",
        "            wordList.append(tempList)\n",
        "\n",
        "        # Append the wordList to the sentence.\n",
        "        sentenceList.append(wordList)\n",
        "\n",
        "    # Convert sentenceList to a Numpy array.\n",
        "    sentenceList = np.asarray(sentenceList)\n",
        "\n",
        "    # Return it.\n",
        "    return np.asarray(sentenceList)\n",
        "\n",
        "# Call the function here\n",
        "train_y1 = to_categorical(train_y, categories = len(tag2idx))\n",
        "print(train_y[0])\n",
        "print(train_y1[0][0])\n",
        "print(train_y1[0][1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 5  1  1  7  1  1  3  8  5  1  7  9  5  1  1 10  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN-Roc_AORIp",
        "outputId": "98923533-f3b3-47fc-e02d-80f426a52b24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Trains the model.\n",
        "def train(model, train_X, train_y1):\n",
        "\n",
        "    # Fit the data into the Keras model, through 40 passes (epochs).\n",
        "    model.fit(train_X, train_y1, batch_size=128, epochs=40, validation_split=0.2)\n",
        "\n",
        "    # Return the model.\n",
        "    return model\n",
        "\n",
        "model = train(model, train_X, train_y1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "359/359 [==============================] - 42s 117ms/step - loss: 0.2035 - accuracy: 0.9408 - val_loss: 0.0374 - val_accuracy: 0.9905\n",
            "Epoch 2/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 0.0185 - accuracy: 0.9950 - val_loss: 0.0152 - val_accuracy: 0.9953\n",
            "Epoch 3/40\n",
            "359/359 [==============================] - 42s 116ms/step - loss: 0.0092 - accuracy: 0.9971 - val_loss: 0.0130 - val_accuracy: 0.9958\n",
            "Epoch 4/40\n",
            "359/359 [==============================] - 41s 116ms/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0123 - val_accuracy: 0.9961\n",
            "Epoch 5/40\n",
            "359/359 [==============================] - 42s 116ms/step - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.0120 - val_accuracy: 0.9962\n",
            "Epoch 6/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 0.0045 - accuracy: 0.9985 - val_loss: 0.0123 - val_accuracy: 0.9962\n",
            "Epoch 7/40\n",
            "359/359 [==============================] - 42s 116ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0127 - val_accuracy: 0.9962\n",
            "Epoch 8/40\n",
            "359/359 [==============================] - 41s 116ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0135 - val_accuracy: 0.9961\n",
            "Epoch 9/40\n",
            "359/359 [==============================] - 42s 116ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0143 - val_accuracy: 0.9961\n",
            "Epoch 10/40\n",
            "359/359 [==============================] - 42s 116ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0151 - val_accuracy: 0.9960\n",
            "Epoch 11/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0160 - val_accuracy: 0.9959\n",
            "Epoch 12/40\n",
            "359/359 [==============================] - 41s 116ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0171 - val_accuracy: 0.9959\n",
            "Epoch 13/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0182 - val_accuracy: 0.9958\n",
            "Epoch 14/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 8.5686e-04 - accuracy: 0.9998 - val_loss: 0.0193 - val_accuracy: 0.9958\n",
            "Epoch 15/40\n",
            "359/359 [==============================] - 42s 116ms/step - loss: 7.0290e-04 - accuracy: 0.9998 - val_loss: 0.0202 - val_accuracy: 0.9957\n",
            "Epoch 16/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 5.6142e-04 - accuracy: 0.9999 - val_loss: 0.0212 - val_accuracy: 0.9957\n",
            "Epoch 17/40\n",
            "359/359 [==============================] - 42s 116ms/step - loss: 4.3604e-04 - accuracy: 0.9999 - val_loss: 0.0223 - val_accuracy: 0.9956\n",
            "Epoch 18/40\n",
            "359/359 [==============================] - 42s 117ms/step - loss: 3.5946e-04 - accuracy: 0.9999 - val_loss: 0.0231 - val_accuracy: 0.9956\n",
            "Epoch 19/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 2.9594e-04 - accuracy: 0.9999 - val_loss: 0.0243 - val_accuracy: 0.9956\n",
            "Epoch 20/40\n",
            "359/359 [==============================] - 41s 114ms/step - loss: 2.4371e-04 - accuracy: 0.9999 - val_loss: 0.0252 - val_accuracy: 0.9955\n",
            "Epoch 21/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 2.1826e-04 - accuracy: 1.0000 - val_loss: 0.0254 - val_accuracy: 0.9956\n",
            "Epoch 22/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 2.1589e-04 - accuracy: 1.0000 - val_loss: 0.0264 - val_accuracy: 0.9956\n",
            "Epoch 23/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 2.3882e-04 - accuracy: 0.9999 - val_loss: 0.0264 - val_accuracy: 0.9956\n",
            "Epoch 24/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 2.4644e-04 - accuracy: 0.9999 - val_loss: 0.0266 - val_accuracy: 0.9956\n",
            "Epoch 25/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 1.9622e-04 - accuracy: 1.0000 - val_loss: 0.0276 - val_accuracy: 0.9956\n",
            "Epoch 26/40\n",
            "359/359 [==============================] - 41s 116ms/step - loss: 1.3239e-04 - accuracy: 1.0000 - val_loss: 0.0273 - val_accuracy: 0.9957\n",
            "Epoch 27/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 1.2024e-04 - accuracy: 1.0000 - val_loss: 0.0284 - val_accuracy: 0.9956\n",
            "Epoch 28/40\n",
            "359/359 [==============================] - 41s 116ms/step - loss: 1.0799e-04 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 0.9956\n",
            "Epoch 29/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 1.0481e-04 - accuracy: 1.0000 - val_loss: 0.0292 - val_accuracy: 0.9956\n",
            "Epoch 30/40\n",
            "359/359 [==============================] - 41s 116ms/step - loss: 2.0422e-04 - accuracy: 0.9999 - val_loss: 0.0288 - val_accuracy: 0.9957\n",
            "Epoch 31/40\n",
            "359/359 [==============================] - 42s 116ms/step - loss: 1.7879e-04 - accuracy: 1.0000 - val_loss: 0.0295 - val_accuracy: 0.9956\n",
            "Epoch 32/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 1.1433e-04 - accuracy: 1.0000 - val_loss: 0.0295 - val_accuracy: 0.9957\n",
            "Epoch 33/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 7.2802e-05 - accuracy: 1.0000 - val_loss: 0.0300 - val_accuracy: 0.9956\n",
            "Epoch 34/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 4.9947e-05 - accuracy: 1.0000 - val_loss: 0.0304 - val_accuracy: 0.9957\n",
            "Epoch 35/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 4.6143e-05 - accuracy: 1.0000 - val_loss: 0.0301 - val_accuracy: 0.9958\n",
            "Epoch 36/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 4.6284e-05 - accuracy: 1.0000 - val_loss: 0.0308 - val_accuracy: 0.9957\n",
            "Epoch 37/40\n",
            "359/359 [==============================] - 41s 115ms/step - loss: 5.2532e-05 - accuracy: 1.0000 - val_loss: 0.0310 - val_accuracy: 0.9957\n",
            "Epoch 38/40\n",
            "359/359 [==============================] - 42s 116ms/step - loss: 2.0952e-04 - accuracy: 0.9999 - val_loss: 0.0306 - val_accuracy: 0.9956\n",
            "Epoch 39/40\n",
            "359/359 [==============================] - 42s 116ms/step - loss: 2.5624e-04 - accuracy: 0.9999 - val_loss: 0.0299 - val_accuracy: 0.9956\n",
            "Epoch 40/40\n",
            "359/359 [==============================] - 42s 116ms/step - loss: 1.1632e-04 - accuracy: 1.0000 - val_loss: 0.0307 - val_accuracy: 0.9956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANPb-K98i0w8",
        "outputId": "722f6df6-7b4f-431f-a962-ae620e49236f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Convert the sentence to one the Keras model can use (integer as words).\n",
        "def convertModelInput(sentence, MAX_LENGTH):\n",
        "\n",
        "    # Create a sentence array.\n",
        "    sentArray = []\n",
        "\n",
        "    # If the sentence is a string convert it into an array.\n",
        "    if type(sentence) == str:\n",
        "        sentence = sentence.lower()\n",
        "        sentArray = sentence.split(' ')\n",
        "        print(sentArray)\n",
        "\n",
        "    # Otherwise, just copy it directly to the sentArray.\n",
        "    elif type(sentence) == list:\n",
        "        sentArray = sentence.copy()\n",
        "        print(sentArray)\n",
        "    \n",
        "    else:\n",
        "        sys.exit(\"Incorrect input to method\")\n",
        "\n",
        "    # Creates a test_X list.\n",
        "    test_X = []\n",
        "\n",
        "    # For each word in the test setence.\n",
        "    for word in sentArray:\n",
        "\n",
        "        # Try finding the word to integer value in the dictionary & append it.\n",
        "        try:\n",
        "            test_X.append(word2idx[word.lower()])\n",
        "\n",
        "        # Otherwise, use an OOV identifier and append it to test_X.\n",
        "        except:\n",
        "            test_X.append(word2idx[\"[OOV]\"])\n",
        "\n",
        "    # Append a 0 to test_X until it the sentence is MAX_LENGTH.\n",
        "    for i in range(MAX_LENGTH - len(sentArray)):\n",
        "        test_X.append(0)\n",
        "\n",
        "    # Return test_X as an Numpy array.\n",
        "    return np.array(test_X)\n",
        "\n",
        "# Test the sentence in the given model.\n",
        "def test(model, sentence):\n",
        "\n",
        "    # Convert the sentence to an integer Numpy array the model can use.\n",
        "    test_X = convertModelInput(sentence, MAX_LENGTH)\n",
        "\n",
        "    # Get the prediction tag results (as an integer list) from the model.\n",
        "    intResults = model.predict_classes(test_X)\n",
        "\n",
        "    # Create an array for the tag (in English).\n",
        "    tagResults = []\n",
        "    \n",
        "    # For each sentence in the results.\n",
        "    for sentence in intResults:\n",
        "\n",
        "        # For each tag in the setence, append the relevand tag from the integer.\n",
        "        for tag in sentence:\n",
        "            tagResults.append(id2tagX[tag])\n",
        "    \n",
        "    # Return the tag list.\n",
        "    return tagResults\n",
        "\n",
        "# For the first evaluation sentence.\n",
        "testString1 = [\"the\", \"secretariat\", \"is\", \"expected\", \"to\", \"race\", \"tomorrow\", \".\"]\n",
        "tag1 = test(model, testString1)\n",
        "print(tag1)\n",
        "print(\"\")\n",
        "\n",
        "# For the second evaluation sentence.\n",
        "testString2 = \"people continue to enquire the reason for the race for outer space .\"\n",
        "tag2 = test(model, testString2)\n",
        "print(tag2)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'secretariat', 'is', 'expected', 'to', 'race', 'tomorrow', '.']\n",
            "['DETERMINER', 'PRONOUN', 'VERB', 'VERB', 'PREPOSITION', 'NOUN', 'NOUN', 'PUNCT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "\n",
            "['people', 'continue', 'to', 'enquire', 'the', 'reason', 'for', 'the', 'race', 'for', 'outer', 'space', '.']\n",
            "['NOUN', 'VERB', 'PREPOSITION', 'NOUN', 'DETERMINER', 'NOUN', 'PREPOSITION', 'DETERMINER', 'NOUN', 'PREPOSITION', 'ADJECTIVE', 'NOUN', 'PUNCT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMXS1UAcGfyO"
      },
      "source": [
        "# Delete the /train/ folder \n",
        "# (Google colab does not the deletion of a folder if it still has contents).\n",
        "!rm -dr /content/train/"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}